[{"authors":null,"categories":null,"content":"üéâ Update: PhD completed (July 2024) ‚Ä¢ Now at NVIDIA.\n I am Ayush Maheshwari (‡§Ü‡§Ø‡•Å‡§∑ ‡§Æ‡§æ‡§π‡•á‡§∂‡•ç‡§µ‡§∞‡•Ä), working as Senior Solutions Architect at NVIDIA in the NVAITC India Team.\nAt NVIDIA, I focus on:\n Foundation Models: Building multilingual and domain-specific foundation models for Indian languages and scientific domains. With additional focus on AI for Science models such as geospatial foundation models, air pollution models, etc. Research Collaborations: Collaborating with academic institutions and research organizations to advance AI applications in science and technology.  My work involves architecting AI solutions, conducting applied research, and enabling the broader research community through technical workshops and collaborations.\nPreviously, I have completed my PhD from CSE, IITB (India) with Prof. Ganesh Ramakrishnan. I was fortunate to be funded by Ekal fellowship from Ekal foundation during my PhD.Prof. Manjesh Kumar Hanawal -- My research interests lie in the area of Natural Language Processing, Graphs from machine learning perspective. I have worked on constrained neural machine translation and semi- and un-supervised machine learning problems with data-programming.\nDuring my PhD, I was a key member of neural machine translation project, UDAAN, which helps publishers to quickly translate technical content in Indian languages. The project is open-source and used by several Indian government technical education agencies and official languages departments. In my spare time, I enjoy playing tabletennis, cricket and reading about Indian culture, Ram√°ya·πáa and Mah√°bh√°rat. \n  Download my resum√© (Last updated: May 2025)\n","date":1764547200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1764547200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"üéâ Update: PhD completed (July 2024) ‚Ä¢ Now at NVIDIA.\n I am Ayush Maheshwari (‡§Ü‡§Ø‡•Å‡§∑ ‡§Æ‡§æ‡§π‡•á‡§∂‡•ç‡§µ‡§∞‡•Ä), working as Senior Solutions Architect at NVIDIA in the NVAITC India Team.\nAt NVIDIA, I focus on:","tags":null,"title":"Ayush Maheshwari","type":"authors"},{"authors":[],"categories":null,"content":"Tutorial: Beyond Transformers - Deep Dive into Mamba and State Space Models\nPresented at the 11th International Conference on Pattern Recognition and Machine Intelligence (PReMI 2025), IIT Delhi.\nThis tutorial session explored:\n State Space Models (SSMs) fundamentals and evolution Mamba architecture - a linear-time sequence modeling alternative to transformers Selective state spaces and their advantages Comparison of SSMs vs Transformers for various sequence modeling tasks Nemotron Nanov3  PReMI 2025 brought together researchers and practitioners in pattern recognition, machine intelligence, and related fields from across the globe.\nConference Date: December 11-14, 2025 Tutorial Date: December 11, 2025 (Pre-conference Tutorial Day)\n","date":1765443600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1765443600,"objectID":"5d279a3d0aab0119cae586672fa53560","permalink":"/talk/tutorial-at-premi-2025-beyond-transformers-deep-dive-into-mamba-and-ssms/","publishdate":"2025-12-01T00:00:00Z","relpermalink":"/talk/tutorial-at-premi-2025-beyond-transformers-deep-dive-into-mamba-and-ssms/","section":"event","summary":"Tutorial session on state space models (SSMs) and Mamba architecture as alternatives to transformer models for sequence modeling.","tags":["Mamba","SSM","State Space Models","Transformers","Deep Learning","Sequence Modeling"],"title":"Tutorial at PReMI 2025 - Beyond Transformers - Deep Dive into Mamba and SSMs","type":"event"},{"authors":["Ayush Maheshwari","Kaushal Sharma","Vivek Patel","Aditya Maheshwari"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1764547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1764547200,"objectID":"5f718f65b1ffe13d677bad3e0a9f38fb","permalink":"/publication/indicparam/","publishdate":"2025-12-01T00:00:00Z","relpermalink":"/publication/indicparam/","section":"publication","summary":"**TL;DR:** Comprehensive benchmark to evaluate LLMs on low-resource Indic languages. Dataset publicly available on HuggingFace. Addresses critical gap in multilingual NLP evaluation.","tags":[],"title":"IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages","type":"publication"},{"authors":["Ayush Maheshwari","Kaushal Sharma","Vivek Patel","Aditya Maheshwari"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1754006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1754006400,"objectID":"904e52d4d73741ad03a6c6768999c178","permalink":"/publication/parambench/","publishdate":"2025-08-01T00:00:00Z","relpermalink":"/publication/parambench/","section":"publication","summary":"**TL;DR:** Graduate-level benchmark for evaluating LLM understanding on Indic subjects. Dataset available on HuggingFace. Addresses need for rigorous multilingual evaluation.","tags":[],"title":"ParamBench: A Graduate-Level Benchmark for Evaluating LLM Understanding on Indic Subjects","type":"publication"},{"authors":["Ayush Maheshwari","Atul Kumar Singh","Karthika NJ","Krishnakant Bhat","Preethi Jyothi","Ganesh Ramakrishnan"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1748736e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748736e3,"objectID":"86f561f72a48123b4533d3b33ef7f969","permalink":"/publication/lexgen-acl/","publishdate":"2025-06-01T00:00:00Z","relpermalink":"/publication/lexgen-acl/","section":"publication","summary":"**TL;DR:** Domain-aware multilingual lexicon generation for 6 Indian languages across 8 domains using routing-based architecture. Released benchmark with 75K+ translation pairs. **Accepted at ACL Main Conference 2025.**","tags":[],"title":"LexGen: Domain-aware Multilingual Lexicon Generation","type":"publication"},{"authors":[],"categories":null,"content":"IndiaAI Official Pre-Summit Event for AI Impact Summit 2026\nDay 2 hands-on technical workshop in collaboration with NVIDIA, covering:\n End-to-end LLM development workflows Optimizing and deploying models on NVIDIA GPUs GPU-based training and efficient model serving Architecture insights \u0026amp; mixed-precision training techniques  Co-presented with Dr. Manish Modani and team from NVIDIA NVAITC.\nBuilt for AI developers, researchers, and engineers eager to scale their models with cutting-edge compute infrastructure.\n","date":1742029200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1742029200,"objectID":"0cb8df41323e276f060d7015c0dd88d7","permalink":"/talk/casml-2025-llm-development-workshop-at-iisc-bangalore/","publishdate":"2025-02-01T00:00:00Z","relpermalink":"/talk/casml-2025-llm-development-workshop-at-iisc-bangalore/","section":"event","summary":"Day 2 workshop on mastering foundations of LLM development on NVIDIA platforms. IndiaAI Official Pre-Summit Event for AI Impact Summit 2026.","tags":["LLM","NVIDIA","GPU","Deep Learning","AI"],"title":"CASML 2025 - LLM Development Workshop at IISc Bangalore","type":"event"},{"authors":["Yaswanth M","Vaibhav Singh","Ayush Maheshwari","Amrith Krishna","Ganesh Ramakrishnan"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1740787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1740787200,"objectID":"c70fb0d6525126375d44c4a430ae73f5","permalink":"/publication/naacl-arise/","publishdate":"2025-03-01T00:00:00Z","relpermalink":"/publication/naacl-arise/","section":"publication","summary":"**TL;DR:** ARISE iteratively induces rules and generates synthetic data for text classification via bootstrapping. Outperforms complex methods like contrastive learning across diverse domains and languages. Published at NAACL 2025 Findings.","tags":[],"title":"ARISE: Iterative Rule Induction and Synthetic Data Generation for Text Classification","type":"publication"},{"authors":[],"categories":null,"content":"Lecture Series on Foundational Models Development - Collaboration with C-DAC\nMulti-session hands-on workshop series covering the complete foundational models (FMs) development lifecycle. Attended by 1000+ participants across research labs, academia and industry. List of sessions delivered by me.\nSeries Topics:\n Cluster Health Check using NCCL and MLPerf Benchmarks Large-scale data curation Distributed and stable training Deploying Foundational Models: Challenges and Best Practices Post-training and Evaluation  This series brings cutting-edge foundational model development to practitioners and researchers across India.\n","date":1734256800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1734256800,"objectID":"03ef1c1eb894c59a7c7d2b6c1f52162b","permalink":"/talk/c-dac-nvidia-foundational-models-lecture-series/","publishdate":"2025-01-15T00:00:00Z","relpermalink":"/talk/c-dac-nvidia-foundational-models-lecture-series/","section":"event","summary":"Multi-session hands-on lecture series on foundational models development lifecycle, powered by PARAM Siddhi-AI supercomputer. Delivered session on 'Deploying Foundational Models - Challenges and Best Practices'.","tags":["Foundational Models","LLM","Deployment","NVIDIA","C-DAC","Supercomputing"],"title":"C-DAC \u0026 NVIDIA Foundational Models Lecture Series","type":"event"},{"authors":["Ayush Maheshwari","Preethi Jyothi","Ganesh Ramakrishnan"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1730419200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730419200,"objectID":"7ffb97251f5da0ee5839889219fd30e5","permalink":"/publication/emnlp-dictdis/","publishdate":"2024-11-01T00:00:00Z","relpermalink":"/publication/emnlp-dictdis/","section":"publication","summary":"**TL;DR:** DictDis disambiguates between multiple dictionary candidate translations in lexically constrained NMT. Achieves 2-3 BLEU point improvements across regulatory, finance, engineering, and health domains. Published at EMNLP 2024 Findings.","tags":[],"title":"DictDis: Dictionary Constrained Disambiguation for Improved NMT","type":"publication"},{"authors":["Divya Jyoti Bajpai","Ayush Maheshwari","Manjesh Kumar Hanawal","Ganesh Ramakrishnan"],"categories":null,"content":"The availability of large annotated data can be a critical bottleneck in training machine learning algorithms successfully, especially when applied to diverse domains. Weak supervision offers a promising alternative by accelerating the creation of labeled training data using domainspecific rules. However, it requires users to write a diverse set of high-quality rules to assign labels to the unlabeled data. Automatic Rule Induction (ARI) approaches circumvent this problem by automatically creating rules from features on a small labeled set and filtering a final set of rules from them. In the ARI approach, the crucial step is to filter out a set of a high-quality useful subset of rules from the large set of automatically created rules. In this paper, we propose an algorithm FAIR (Filtering of Automatically Induced Rules) to filter rules from a large number of automatically induced rules using submodular objective functions that account for the collective precision, coverage, and conflicts of the rule set. We experiment with three ARI approaches and five text classification datasets to validate the superior performance of our algorithm with respect to several semi-supervised label aggregation approaches. Further, we show that FAIR achieves statistically significant results in comparison to existing rule-filtering approaches. The source code is available at https://github.com/ayushbits/FAIR-LF-Induction.\n Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/).\r--\r","date":1709251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709251200,"objectID":"0fe610100a9164f8f20422b7ee7b4010","permalink":"/publication/fair_lfs_induction/","publishdate":"2024-03-01T00:00:00Z","relpermalink":"/publication/fair_lfs_induction/","section":"publication","summary":"**TL;DR:** FAIR filters automatically induced rules using submodular optimization that accounts for precision, coverage, and conflicts. Outperforms existing rule-filtering approaches with statistically significant results. Published at EACL 2024.","tags":[],"title":"FAIR: Filtering of Automatically Induced Rules","type":"publication"},{"authors":["Ayush Maheshwari","Ashim Gupta","Amrith Krishna","Atul Kumar Singh","Ganesh Ramakrishnan","G. Anil Kumar","Jitin Singla"],"categories":null,"content":"We release SƒÅmayik, a dataset of around 53,000 parallel English-Sanskrit sentences, written in contemporary prose. Sanskrit is a classical language still in sustenance and has a rich documented heritage. However, due to the limited availability of digitized content, it still remains a low-resource language. Existing Sanskrit corpora, whether monolingual or bilingual, have predominantly focused on poetry and offer limited coverage of contemporary written materials. SƒÅmayik is curated from a diverse range of domains, including language instruction material, textual teaching pedagogy, and online tutorials, among others. It stands out as a unique resource that specifically caters to the contemporary usage of Sanskrit, with a primary emphasis on prose writing. Translation models trained on our dataset demonstrate statistically significant improvements when translating out-of-domain contemporary corpora, outperforming models trained on older classical-era poetry datasets. Finally, we also release benchmark models by adapting four multilingual pre-trained models, three of them have not been previously exposed to Sanskrit for translating between English and Sanskrit while one of them is multi-lingual pre-trained translation model including English and Sanskrit. The dataset and source code is present at https://github.com/ayushbits/saamayik.\n Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/).\r--\r","date":1709251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709251200,"objectID":"e91b45e2e521b57d46dfd0f2a52750d1","permalink":"/publication/saamayik/","publishdate":"2024-03-01T00:00:00Z","relpermalink":"/publication/saamayik/","section":"publication","summary":"**TL;DR:** First comprehensive benchmark and dataset for English-Sanskrit machine translation. Addresses critical gap in classical language NLP. Published at LREC-COLING 2024.","tags":[],"title":"SƒÅmayik: A Benchmark and Dataset for English-Sanskrit Translation","type":"publication"},{"authors":[],"categories":null,"content":"Workshop: Building Indian Language Foundation Models\nPresented at NASSCOM‚Äôs National Leadership and Technology Forum (NLTF 2024), India‚Äôs premier platform for technology and business leadership.\nPresentation Overview Shared insights and technical approaches from building large-scale foundation models specifically designed for Indian languages, addressing the unique challenges of linguistic diversity in India.\nKey Topics Covered: 1. Data Collection \u0026amp; Processing:\n Large-scale multilingual data curation strategies Quality control for diverse Indian language data Handling code-mixing and transliteration challenges Building evaluation datasets for low-resource languages  2. Model Architecture \u0026amp; Training:\n Tokenizer design for morphologically rich Indian languages Training architecture for multilingual models Distributed training on large accelerator clusters Optimization techniques for efficient training  3. Model Tuning \u0026amp; Deployment:\n Instruction tuning approaches Preference training for alignment Deployment considerations for production systems Performance evaluation across language families  4. Real-world Impact:\n Applications in education, government services, and content creation Bridging the digital divide through vernacular AI Democratizing access to AI technology across India  Context This work was conducted while leading a team of 5 researchers building Indic large language models from scratch, combining technical innovation with practical deployment considerations for India‚Äôs multilingual landscape.\nThe presentation contributed to NASSCOM‚Äôs vision of positioning India as a global AI powerhouse.\n Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\r- **Create** slides using Wowchemy\u0026#39;s [_Slides_](https://wowchemy.com/docs/managing-content/#create-slides) feature and link using `slides` parameter in the front matter of the talk file\r- **Upload** an existing slide deck to `static/` and link using `url_slides` parameter in the front matter of the talk file\r- **Embed** your slides (e.g. Google Slides) or presentation video on this page using [shortcodes](https://wowchemy.com/docs/writing-markdown-latex/).\rFurther event details, including [page elements](https://wowchemy.com/docs/writing-markdown-latex/) such as image galleries, can be added to the body of this page. --\r","date":1708344e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708344e3,"objectID":"2de1692306561907bdc1d7774db54622","permalink":"/talk/workshop-presentation-at-nasscom-nltf-2024-building-indian-language-foundation-models/","publishdate":"2024-02-20T00:00:00Z","relpermalink":"/talk/workshop-presentation-at-nasscom-nltf-2024-building-indian-language-foundation-models/","section":"event","summary":"Presented work on building large-scale foundation models for Indian languages, covering data collection, training architecture, and deployment challenges for multilingual LLMs.","tags":["Foundation Models","LLM","Indic Languages","Multilingual NLP","AI","Deep Learning"],"title":"Workshop Presentation at NASSCOM NLTF 2024 - Building Indian Language Foundation Models","type":"event"},{"authors":["Abhishek Singh","Venkatapathy Subramanian","Ayush Maheshwari","Pradeep Narayan","Devi Prasad Shetty","Ganesh Ramakrishnan"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1698796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698796800,"objectID":"2bed93a3b8e19b7057a78350e6154880","permalink":"/publication/eigen-ml4health/","publishdate":"2023-11-01T00:00:00Z","relpermalink":"/publication/eigen-ml4health/","section":"publication","summary":"**TL;DR:** EIGEN combines expert knowledge with joint learning for high-fidelity information extraction from medical document images. Applied to healthcare domain at ML4Health (NeurIPS) 2023.","tags":[],"title":"EIGEN: Expert-Informed Joint Learning Aggregation for High-Fidelity Information Extraction from Document Images","type":"publication"},{"authors":[],"categories":null,"content":"Invited Tutorial: Data Efficient Machine Learning for Educational Content Creation\nCo-presented with Prof. Ganesh Ramakrishnan at the 16th International Conference on Educational Data Mining (EDM 2023), IISc Bangalore.\nTutorial Overview This half-day tutorial showcased practical applications of machine learning in education, specifically focusing on neural machine translation (NMT) for making educational content accessible across multilingual societies.\nKey Topics Covered:   Data-efficient NMT techniques for low-resource educational content\n  Domain-specific translation challenges in technical/higher education textbooks\n  UDAAN translation ecosystem - our production system that has:\n Translated 100+ diploma and engineering books Supported 11+ Indian languages Empowered 100+ professional translators Received Presidential recognition    Lexicon adherence and terminology consistency in technical translation\n  Post-editing workflows and human-in-the-loop systems\n  Real-world deployment insights from large-scale educational content translation\n  Impact The UDAAN project demonstrates how ML can bridge language barriers in education, enabling access to technical knowledge for millions of students across India. This work received the Best Paper Award at CODS-COMAD 2023.\nTutorial materials and insights from translating hundreds of technical books across diverse Indian languages.\n Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\r- **Create** slides using Wowchemy\u0026#39;s [_Slides_](https://wowchemy.com/docs/managing-content/#create-slides) feature and link using `slides` parameter in the front matter of the talk file\r- **Upload** an existing slide deck to `static/` and link using `url_slides` parameter in the front matter of the talk file\r- **Embed** your slides (e.g. Google Slides) or presentation video on this page using [shortcodes](https://wowchemy.com/docs/writing-markdown-latex/).\rFurther event details, including [page elements](https://wowchemy.com/docs/writing-markdown-latex/) such as image galleries, can be added to the body of this page. --\r","date":1689325200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689325200,"objectID":"1a0a0ede698d0ad192e5fd794c45b262","permalink":"/talk/invited-tutorial-at-edm-2023-data-efficient-machine-learning-for-educational-content-creation/","publishdate":"2023-04-25T00:00:00Z","relpermalink":"/talk/invited-tutorial-at-edm-2023-data-efficient-machine-learning-for-educational-content-creation/","section":"event","summary":"Half-day invited tutorial on data-efficient machine learning for educational content creation, featuring the UDAAN translation ecosystem that has translated 100+ technical books across 11+ Indian languages.","tags":["Machine Translation","Educational Data Mining","NMT","UDAAN","Data Efficiency","Multilingual NLP"],"title":"Invited Tutorial at EDM 2023 - Data Efficient Machine Learning for Educational Content Creation","type":"event"},{"authors":["Ayush Maheshwari","Ajay Ravindran","Venkatapathy Subramanian","Ganesh Ramakrishnan"],"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"f5d8766f1eb9a17cc63723c7aa988eea","permalink":"/publication/udaan-cods/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/publication/udaan-cods/","section":"publication","summary":"**TL;DR:** A production-ready MT post-editing tool used by 100+ translators to translate technical content into Indian languages. Won Best Paper Award at CODS-COMAD 2023. **Impact:** First batch of engineering books translated using UDAAN were released by the President of India.","tags":["best paper"],"title":"UDAAN - Machine Learning based Post-Editing tool for Document Translation","type":"publication"},{"authors":[],"categories":null,"content":"Presented Two Papers at EMNLP 2022\nAttended the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP) in Abu Dhabi, where I presented two research papers published in EMNLP 2022 Findings.\nPaper 1: Post-OCR Text Correction in Sanskrit Title: ‚ÄúA Benchmark and Dataset for Post-OCR Text Correction in Sanskrit‚Äù\nKey Contributions:\n Released multi-domain benchmark with 218K sentences (1.5M words) from 30 different books Covered diverse domains: astronomy, medicine, mathematics (texts up to 18 centuries old) Dataset spans Sanskrit‚Äôs linguistic and stylistic diversity across 3 millennia Best model (Byt5+SLP1) achieved 23% improvement over OCR output Open-source dataset enabling digitization of 30 million extant Sanskrit manuscripts  Impact: Addressing the digital resource gap for Sanskrit, a classical language with massive manuscript collections.\nPaper 2: SPEAR Data Programming Library Title: ‚ÄúSPEAR: Semi-supervised Data Programming in Python‚Äù (System Demonstration)\nKey Features:\n Open-source Python library for programmatic data labeling Reduces manual annotation effort through weak supervision Implements cutting-edge approaches: Snorkel, ImplyLoss, Learning to Reweight Integrates semi-supervised learning for efficient training 100+ GitHub stars and wide community adoption  Impact: Enabling practitioners to build training datasets efficiently without extensive manual labeling.\nBoth papers address critical challenges in making NLP more accessible and efficient for low-resource scenarios.\n Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\r- **Create** slides using Wowchemy\u0026#39;s [_Slides_](https://wowchemy.com/docs/managing-content/#create-slides) feature and link using `slides` parameter in the front matter of the talk file\r- **Upload** an existing slide deck to `static/` and link using `url_slides` parameter in the front matter of the talk file\r- **Embed** your slides (e.g. Google Slides) or presentation video on this page using [shortcodes](https://wowchemy.com/docs/writing-markdown-latex/).\rFurther event details, including [page elements](https://wowchemy.com/docs/writing-markdown-latex/) such as image galleries, can be added to the body of this page. --\r","date":1670331600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670331600,"objectID":"18da58ce69700e07e545184ae4cfd32e","permalink":"/talk/paper-presentations-at-emnlp-2022-post-ocr-correction-and-data-programming/","publishdate":"2022-11-12T00:00:00Z","relpermalink":"/talk/paper-presentations-at-emnlp-2022-post-ocr-correction-and-data-programming/","section":"event","summary":"Presented two papers at EMNLP 2022 Findings - a benchmark for post-OCR text correction in Sanskrit and a demonstration of SPEAR data programming library.","tags":["OCR","Sanskrit","Data Programming","SPEAR","NLP","Digital Humanities","Weak Supervision"],"title":"Paper Presentations at EMNLP 2022 - Post-OCR Correction and Data Programming","type":"event"},{"authors":["Sai Abhishek","Harshad Ingole","Parth Laturia","Vineeth Dorna","Ayush Maheshwari","Rishabh Iyer","Ganesh Ramakrishnan"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669852800,"objectID":"864355da53687e0140fadc5ac4c0e2a3","permalink":"/publication/spear-demo-emnlp/","publishdate":"2022-10-01T00:00:00Z","relpermalink":"/publication/spear-demo-emnlp/","section":"publication","summary":"**TL;DR:** Python library for programmatic data labeling using weak supervision. Reduces manual labeling effort by combining multiple weak labeling sources (rules, heuristics, models). Open-source toolkit with 100+ GitHub stars.","tags":[],"title":"SPEAR: Semi-supervised Data Programming in Python","type":"publication"},{"authors":["Durga S","Ayush Maheshwari","Pradeep Shenoy","Prathosh AP","Ganesh Ramakrishnan"],"categories":null,"content":"Apart from the standard supervised learning using hard labels, often auxiliary losses are used in many supervised learning settings to improve the model‚Äôs generalisation. For example, knowledge distillation adds a second, teacher mimicking loss to the training of a model, where the teacher may be a pretrained model that outputs a richer distribution over labels. Similarly, in settings with limited labelled data, weak labelling information is used in form of labelling functions. Auxiliary losses are introduced here to combat labelling functions that may be noisy rule-based approximations of true labels. We tackle the problem of learning to combine these losses in a principled manner. We introduce AMAL which learns instance-specific weights using meta learning on a validation metric to achieve optimal mixing of losses. Experiments in a number of knowledge distillation and rule denoising domains show that AMAL provides noticeable gains over competitive baselines in those domains. We empirically analyze our method and share insights into the mechanisms through which it provides performance gains.\n","date":1668816e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668816e3,"objectID":"20a8a93864c2eec56294862180a0cbce","permalink":"/publication/reweighing-loss-aaai23/","publishdate":"2022-11-19T00:00:00Z","relpermalink":"/publication/reweighing-loss-aaai23/","section":"publication","summary":"**TL;DR:** AMAL learns instance-specific weights using meta-learning to optimally combine auxiliary losses in supervised learning. Provides significant gains in knowledge distillation and rule denoising. Published at AAAI 2023.","tags":[],"title":"Reweighing auxiliary losses in supervised learning","type":"publication"},{"authors":["Ayush Maheshwari","Nikhil Singh","Amrith Krishna","Ganesh Ramakrishnan"],"categories":null,"content":"Sanskrit is a classical language with about 30 million extant manuscripts fit for digitisation, available in written, printed or scanned-image forms. However, it is still considered to be a low-resource language when it comes to available digital resources. In this work, we release a post-OCR text correction dataset containing around 218,000 sentences, with 1.5 million words, from 30 different books. Texts in Sanskrit are known to be diverse in terms of their linguistic and stylistic usage since Sanskrit was the `lingua francua‚Äô for discourse in the Indian subcontinent for about 3 millennia. Keeping this in mind, we release a multi-domain dataset, from areas as diverse as astronomy, medicine and mathematics, with some of them as old as 18 centuries. Further, we release multiple strong baselines as benchmarks for the task, based on pre-trained Seq2Seq language models. We find that our best-performing model, consisting of byte level tokenization in conjunction with phonetic encoding (Byt5+SLP1), yields a 23% point increase over the OCR output in terms of word and character error rates. Moreover, we perform extensive experiments in evaluating these models on their performance and analyse common causes of mispredictions both at the graphemic and lexical levels. Our code and dataset is publicly available at https://github.com/ayushbits/pe-ocr-sanskrit.\n Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --\r","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"38287311be5f20c788cc11a57a3f9601","permalink":"/publication/ocr-sanskrit-emnlp/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/publication/ocr-sanskrit-emnlp/","section":"publication","summary":"**TL;DR:** Multi-domain benchmark with 218K sentences for post-OCR correction in Sanskrit. Best model achieves 23% improvement over OCR output. Dataset spans 30 books across astronomy, medicine, and mathematics. Published at EMNLP 2022 Findings.","tags":[],"title":"A Benchmark and Dataset for Post-OCR text correction in Sanskrit","type":"publication"},{"authors":["Ayush Maheshwari","Krishnateja Killamsetty","Ganesh Ramakrishnan","Rishabh Iyer","Marina Danilevsky","Lucian Popa"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"5f6993fc0346d8f8bc46500f65f30087","permalink":"/publication/aggregate-lf-acl/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/publication/aggregate-lf-acl/","section":"publication","summary":"**TL;DR:** Proposes robust aggregation of noisy labeling functions for semi-supervised data programming. Improves weak supervision quality by learning to weight unreliable labels. Published at ACL 2022 Findings.","tags":[],"title":"Learning to Robustly Aggregate Labeling Functions for Semi-supervised Data Programming","type":"publication"},{"authors":null,"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"93c9389e76f8444b9612a7ab06fafbcf","permalink":"/project/udaan/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/project/udaan/","section":"project","summary":"An end-to-end Machine Translation and post-editing platform that has translated 50 books across 10 Indian languages. **100+ professional translators** actively use UDAAN to upload documents, obtain MT output, and edit translations. Received **Presidential recognition** and **Best Paper Award at CODS-COMAD 2023**. Includes 100 digitized dictionaries freely available from CSTT.","tags":["nmt","post-editing","translation","machine learning"],"title":"UDAAN - An NMT pipeline + Post-editing tool to translate document (Best Paper Award at CODS-COMAD 2023)","type":"project"},{"authors":null,"categories":null,"content":"","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"8fae2736be87fed742d79f362205f794","permalink":"/project/spear/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/project/spear/","section":"project","summary":"A widely-adopted **open-source Python library** for programmatic data labeling with **100+ GitHub stars**. SPEAR reduces data labeling efforts by implementing cutting-edge data programming approaches (Snorkel, ImplyLoss, Learning to Reweight). Integrates semi-supervised learning for efficient training and inference. Featured at EMNLP 2021.","tags":["data programming","machine learning"],"title":"SPEAR - Programmatically label and quickly build training data","type":"project"},{"authors":["Atul Sahay","Anshul Nasery","Ayush Maheshwari","Ganesh Ramakrishnan","Rishabh Iyer"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1627776e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776e3,"objectID":"35fea8c5712b3e91029772a16ac0911b","permalink":"/publication/rules-short-acl/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/publication/rules-short-acl/","section":"publication","summary":"**TL;DR:** Augments unsupervised constituency parsing with linguistic rules to improve performance. Combines data-driven and rule-based approaches for better syntactic parsing. Published at ACL 2021 Findings.","tags":[],"title":"Rule Augmented Unsupervised Constituency Parsing","type":"publication"},{"authors":["Ayush Maheshwari","Oishik Chatterjee","Krishnateja Killamsetty","Ganesh Ramakrishnan","Rishabh Iyer"],"categories":null,"content":"The paradigm of data programming, which uses weak supervision in the form of rules/labelling functions, and semi-supervised learning, which augments small amounts of labelled data with a large unlabelled dataset, have shown great promise in several text classification scenarios. In this work, we argue that by not using any labelled data, data programming based approaches can yield sub-optimal performances, particularly when the labelling functions are noisy. The first contribution of this work is an introduction of a framework, SPEAR which is a semi-supervised data programming paradigm that learns a joint model that effectively uses the rules/labelling func- tions along with semi-supervised loss functions on the feature space. Next, we also study SPEAR-SS which additionally does subset selection on top of the joint semi-supervised data programming objective and selects a set of examples that can be used as the labelled set by SPEAR . The goal of SPEAR-SS is to ensure that the labelled data can complement the labelling functions, thereby benefiting from both data-programming as well as appropriately selected data for human labelling. We demonstrate that by effectively combining semi-supervision, data-programming, and sub- set selection paradigms, we significantly out-perform the current state-of-the-art on seven publicly available datasets. The source code is available https://github.com/ayushbits/Semi-Supervised-LFs-Subset-Selection.\n Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --\r","date":1627776e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776e3,"objectID":"cc28ecc9c01b1710d994e116d8e44cd8","permalink":"/publication/semi-dp-acl/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/publication/semi-dp-acl/","section":"publication","summary":"**TL;DR:** SPEAR combines semi-supervised learning with data programming to improve noisy labeling functions. Significantly outperforms state-of-the-art on seven datasets by jointly learning from rules and labeled data. Published at ACL 2021 Findings.","tags":[],"title":"Semi-Supervised Data Programming with Subset Selection,","type":"publication"},{"authors":["Soumya Chatterjee","Ayush Maheshwari","Ganesh Ramakrishnan","Saketha Nath Jagaralpudi"],"categories":null,"content":"We consider the problem of multi-label classification, where the labels lie in a hierarchy. However, unlike most existing works in hierarchical multi-label classification, we do not assume that the label-hierarchy is known. Encouraged by the recent success of hyperbolic embeddings in capturing hierarchical relations, we propose to jointly learn the classifier parameters as well as the label embeddings. Such a joint learning is expected to provide a two-fold advantage: i) the classifier generalises better as it leverages the prior knowledge of existence of a hierarchy over the labels, and ii) in addition to the label co-occurrence information, the label-embedding may benefit from the manifold structure of the input datapoints, leading to embeddings that are more faithful to the label hierarchy. We propose a novel formulation for the joint learning and empirically evaluate its efficacy. The results show that the joint learning improves over the baseline that employs label co-occurrence based pre-trained hyperbolic embeddings. Moreover, the proposed classifiers achieve state-of-the-art generalization on standard benchmarks. We also present evaluation of the hyperbolic embeddings obtained by joint learning and show that they represent the hierarchy more accurately than the other alternatives. The source code of the paper is available at https://github.com/soumyac1999/hyperbolic-label-emb-for-hmc\n Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/).\r--\r","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"05556f6c530ce4f9e64cf0c693c93112","permalink":"/publication/joint_learning_hyperbolic/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/publication/joint_learning_hyperbolic/","section":"publication","summary":"**TL;DR:** Jointly learns classifier parameters and hyperbolic label embeddings for hierarchical multi-label classification without assuming known label hierarchy. Achieves state-of-the-art results by capturing hierarchical structure in hyperbolic space. Published at EACL 2021.","tags":[],"title":"Joint Learning of Hyperbolic Label Embeddings for Hierarchical Multi-label Classification","type":"publication"},{"authors":["Atul Sahay","Ayush Maheshwari","Ritesh Kumar","Ganesh Ramakrishnan","Manjesh Kumar Hanawal","Kavi Arya"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"80781710f32c9dc960b2e861bce3fc9a","permalink":"/publication/parsetree-ijcnn/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/publication/parsetree-ijcnn/","section":"publication","summary":"**TL;DR:** Unsupervised learning of explainable parse trees to improve model generalization in NLP tasks. Published at IJCNN 2021.","tags":[],"title":"Unsupervised Learning of Explainable Parse Trees for Improved Generalisation","type":"publication"},{"authors":null,"categories":null,"content":"import libr print(\u0026#39;hello\u0026#39;) Overview  The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It‚Äôs a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more    \nGet Started  üëâ Create a new site üìö Personalize your site üí¨ Chat with the Wowchemy community or Hugo community üê¶ Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy üí° Request a feature or report a bug for Wowchemy ‚¨ÜÔ∏è Updating Wowchemy? View the Update Tutorial and Release Notes  Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n‚ù§Ô∏è Click here to become a sponsor and help support Wowchemy‚Äôs future ‚ù§Ô∏è As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features ü¶Ñ‚ú®\nEcosystem  Hugo Academic CLI: Automatically import publications from BibTeX  Inspiration Check out the latest demo of what you‚Äôll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures  Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, ‰∏≠Êñá, and Portugu√™s Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome üëã We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":null,"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["Ayush Maheshwari","Hrishikesh Patel","Nandan Rathod","Ritesh Kumar","Ganesh Ramakrishnan","Pushpak Bhattacharyya"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":159624e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":159624e4,"objectID":"2c3ad6d6a5b3b36de9099d2cd67c0108","permalink":"/publication/tail-starai/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/publication/tail-starai/","section":"publication","summary":"**TL;DR:** Rule-augmented sequence labeling approach for event extraction that handles tail/rare event types. Presented at StarAI Workshop (AAAI 2020).","tags":[],"title":"Tale of tails using rule augmented sequence labeling for event extraction","type":"publication"},{"authors":["Rohit Saluja","Ayush Maheshwari","Ganesh Ramakrishnan","Parag Chaudhuri","Mark Carman"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"f92db2b804e935774cd2cc414b6d7f23","permalink":"/publication/ocr-icdar/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/publication/ocr-icdar/","section":"publication","summary":"**TL;DR:** Robust end-to-end OCR systems for reading license plates and street signs in challenging real-world conditions. Practical application of computer vision for automated text recognition. Published at ICDAR 2019.","tags":[],"title":"OCR On-the-Go: Robust End-to-end Systems for Reading License Plates and Street Signs","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1569888e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888e3,"objectID":"5b58e541ee07c36884eb518b795a6f58","permalink":"/project/tofi/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/project/tofi/","section":"project","summary":"Temples of India is a not-for-profit knowledge platform to document and store possibly all details of temples across Indian subcontinent. We aim to present each detail related to the temple such as its location, images of the temple, videos, open and close timings, etc.","tags":["Information Retrieval","machine learning"],"title":"Temples of India","type":"project"},{"authors":["Ayush Maheshwari","Ayush Goyal","Manjesh Kumar Hanawal","Ganesh Ramakrishnan"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"34fa28765f4f4a4a363315d4889d5ba8","permalink":"/publication/dyngan/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/publication/dyngan/","section":"publication","summary":"**TL;DR:** Uses GANs for dynamic network embedding to capture temporal evolution of networks. Presented at NeurIPS Graph Representation Learning Workshop 2019.","tags":[],"title":"DynGAN: Generative Adversarial Networks for Dynamic Network Embedding","type":"publication"},{"authors":["Ayush Maheshwari"],"categories":[],"content":"from IPython.core.display import Image Image(\u0026#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png\u0026#39;)    print(\u0026#34;Welcome to Academic!\u0026#34;) Welcome to Academic!\r Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata (front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post\u0026#39;s title date: 2019-09-01 # Put any other Academic metadata here... --- Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post‚Äôs folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=. Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":["Ayush Maheshwari","Ayush Goyal","Manjesh Kumar Hanawal","Ganesh Ramakrishnan"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"b0beabdc5c49ee2bdba3fdf884ed3f35","permalink":"/publication/rep_learning/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/publication/rep_learning/","section":"publication","summary":"**TL;DR:** Integrates content and structure information for representation learning on graphs. Published at COMSNETS 2019.","tags":[],"title":"Representation Learning on Graphs by Integrating Content and Structure Information","type":"publication"},{"authors":["Ayush Maheshwari","Vishwajeet Kumar","Ganesh Ramakrishnan","J. Saketha Nath"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541030400,"objectID":"aed9df117048c5bf0272e3a7db990b1d","permalink":"/publication/temples-demo/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/publication/temples-demo/","section":"publication","summary":"**TL;DR:** Entity resolution and location disambiguation for Hindu temples using web data. Demo system presented at NAACL 2018.","tags":[],"title":"Entity Resolution and Location Disambiguation in the Ancient Hindu Temples Domain using Web Data","type":"publication"},{"authors":["Ayush Maheshwari","Kamal Kumar Murari","T. Jayaraman"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"9841257bbefbcc56dcd3287755832910","permalink":"/publication/elec-pune/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/publication/elec-pune/","section":"publication","summary":"**TL;DR:** Extreme value analysis of peak electricity demand and global warming effects in Pune industrial and residential areas. Climate change impact study.","tags":[],"title":"Peak Electricity Demand and Global Warming in the Industrial and Residential areas of Pune : An Extreme Value Approach","type":"publication"},{"authors":null,"categories":null,"content":"\r  [Dec 25] Paper on benchmarking LLMs on extremely-low resource Indic languages is on Arxiv and Huggingface [IndicParam: Benchmark to evaluate LLMs on low-resource Indic langauges]\n  [May 25] Our paper on domain aware lexicon generation is accepted at ACL Main Conference 2025 ‚ù§Ô∏è üòä [LexGen: Domain aware multilingual lexicon generation]\n  [Jan 25] Our paper on synthetic data generation, ARISE: Iterative Rule Induction and Synthetic Data Generation for Text Classification is accepted at NAACL 2025 üéÜüéá [Paper]\n  [Nov 24] Delighted to announce that my PhD work is awarded with Impactful Research Award 2023 by IIT Bombay ‚ù§Ô∏è üéá!\n  [Oct 24] Joined NVIDIA üòå! Excited to be a part of the future of AI üòá\n  [Sep 24] Our paper on Dictionary Constrained Disambiguation for Improved NMT is accepted at EMNLP 2024 [Paper] ‚ù§Ô∏è üòå\n  [July 24] I have successfully defended my PhD thesis on Knowledge Integration in Language Processing Models using Constraint Ingestion and Generation üòáüòäüéÜ {Arxiv soon!}\n  [July 24] Our paper on ‚ÄòEnhancing Low-Resource NMT with a Multilingual Encoder and Knowledge Distillation‚Äô will be presented at LoResMT workshop at ACL 2024 üéâ.\n  [Feb 24] Our paper on development of English - Sanskrit parallel corpus is accepted at LREC-COLING 2024. [Pre-print]\n  [Jan 24] Our paper on ‚ÄòFiltering of automatically induced rules for weak supervision‚Äô accepted at EACL 2024 ‚ù§Ô∏è. [Paper]\n  [Nov 23] Paper accepted at Machine Learning for Health Conference (co-located with Neurips) [Paper] ü•≥\n  üìù Serving in the PC for ARR Industry Track 2022 - Present.\n  üìù Serving in the PC for ACL ARR, 2022 - Present.\n  [Jul 23] üé§ Along with Prof. Ganesh Ramakrishnan I delivered an invited half-day tutorial at Educational Data Mining Conference held at IISc Bangalore. More details in Talk section below ! ü§©\n  [Mar 23] üìöÔ∏è First batch of engineering books translated in Malayalam using our post-editing tool was released by Honourable President of India in Thiruvananthapurm, Kerala. More details on this page.\n  [Jan 23] üèÜÔ∏è Our paper on translation post-editing tool won the best paper award at CoDS-COMAD 2023 ü•≥.\n  [Dec 22] üòç All India Council for Technical Education(AICTE) acknowledged our team of UDAAN project from IIT Bombay, for providing our end-to-end Machine Translation Framework that includes extensive use of lexical resources, a human-in-the-loop machine learning based post-editing platform, etc. which is being used by AICTE for speedy translation of 100s of textbooks into multiple Indian languages. Glad to be a part of the team üòá !\n  üìù Served in the PC for ACL 2021, ACL 2022 and EACL 2022\n  [Nov 22] Our work on Reweighing auxiliary losses in supervised learning accepted at AAAI 2023 üòá.\n  [Nov 22] ü§© Received grant from EMNLP D\u0026amp;I to travel and present our work at EMNLP 2022 in Abu Dhabi ‚úàÔ∏è. Thanks EMNLP D\u0026amp;I !\n  [Nov 22] Demo paper on our in-house translation workbench for efficient translation of documents accepted at CODS-COMAD 2023.\n  [Oct 22] Our NMT work is available on pre-print DICTDIS: Dictionary Constrained Disambiguation for Improved NMT\n  [Oct 22] Our post-editing work on OCR in Sanskrit langugage accepted at EMNLP 2022.\n  ","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"a0812ae5f3c926fea6faf4472cefc8e2","permalink":"/news/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/news/","section":"","summary":"List of news.\r\n","tags":[],"title":"Updates","type":"page"},{"authors":null,"categories":null,"content":" Features  Supports speaker notes   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\nPress Space to play!\n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n- Only the speaker can read these notes - Press `S` key to view Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links    Custom CSS Example Let‚Äôs make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"Features  Supports speaker notes   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export   Code Highlighting Inline code: variable","tags":null,"title":"","type":"slides"}]